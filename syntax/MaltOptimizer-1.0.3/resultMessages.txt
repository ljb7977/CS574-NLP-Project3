-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 1: DATA ANALYSIS
In order to optimize MaltParser for your training set, MaltOptimizer will 
first analyze the data and set some basic parameters.
-----------------------------------------------------------------------------
DATA VALIDATION
Validating the CoNLL data format ...  (may take a few minutes)
Your training set is in valid CoNLL format, but the validation script
gave some warnings, so you may want to consult the logfile
 /mnt/c/Users/JB/Documents/PA3/syntax/MaltOptimizer-1.0.3/logValidationFile.txt .
-----------------------------------------------------------------------------
DATA CHARACTERISTICS
Your training set consists of 20841 sentences and 446451 tokens.
Testing Java Heap ... 
MaltOptimizer has inferred that MaltParser needs at least 7Gb of free memory.
CPOSTAG and POSTAG are identical in your training set.
The LEMMA column is used in your training set.
The FEATS column is used in your training set.
Your training set contains a substantial amount of non-projective trees (24.62 %).
Your training set does not contain unattached internal punctuation.
Your training set has multiple DEPREL labels for tokens where HEAD=0:
ROOT: 91.80% 
PUNC: 8.197% 
-----------------------------------------------------------------------------
BASIC OPTIMIZATION SETUP 
Generating training and test files for optimization...
Five cross-validation folds generated.
Generated training set (373796 tokens) and devtest set (93495 tokens).
Given that your data set is relatively large, we recommend using a single 
development set during subsequent optimization phases. If you prefer to use 5-fold cross-validation, you can specify this instead (-v cv).
Testing the default settings ... (may take a few seconds)
LAS with default settings: 91.58%
Testing root labels ... 
ROOT
PUNC
Default root label reset: -grl ROOT
91.58
91.58
-----------------------------------------------------------------------------
MaltOptimizer has completed the analysis of your training set and saved the
results for future use in /phase1_logFile.txt. Updated MaltParser options can be found
in /phase1_optFile.txt. If you want to change any of these options, you should
edit /phase1_optFile.txt before you start the next optimization phase.

To proceed with Phase 2 (Parsing Algorithm) run the following command:
java -jar MaltOptimizer.jar -p 2 -m <malt_path> -c <trainingCorpus>
-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 2: PARSING ALGORITHM SELECTION

MaltOptimizer found in Phase 1 that your training set contains
a substantial amount of non-projective trees and will therefore 
only try non-projective algorithms.

Testing the non-projective algorithms ...

               CovingtonNonProjective --vs-- StackLazy
                          /                     \
                         /                       \
                        /                         \
                       /                           \
                      /                             \
                     /                               \
                    /                                 \
               NivreEager+PP             StackEager --vs-- StackProjective+PP
                    |                                  |
                    |                                  |
         CovingtonProjective+PP                 NivreStandard+PP


Testing the Covington-Non-Projective algorithm ...
Testing the StackLazy algorithm ...
Testing the NivreEager algorithm with pseudo-projective parsing (PP) ...
New best algorithm: nivreeager
Incremental LAS improvement: + 0.659% (92.24%)
Testing the Covington-Projective algorithm with pseudo-projective parsing (PP) ...
Best Non-Projective algorithm: NivreEager

-----------------------------------------------------------------------------
MaltOptimizer found that the best parsing algorithm is: nivreeager+ pp option
Root Handling testing ...
91.55
92.24
91.55
(normal)
Root handling selected strategy:
	allow_root: true
	allow_reduce: false
Testing pseudo-projective (PP) options ...
New best pp option:path
Incremental LAS improvement: + 0.050% (92.29%)
Default marking strategy reset: path
Incremental improvement over the baseline at the end of Phase 2: +0.710% (92.29%) 
-----------------------------------------------------------------------------
MaltOptimizer has completed the parsing algorithm selection phase for your
training set and saved the results for future use in phase2_logFile.txt. 
Updated MaltParser options can be found in phase2_optFile.txt. If you want
to change any of these options, you should edit phase2_optFile.txt before.
you start the next optimization phase.

To proceed with Phase 3 (Feature Selection) run the following command:
java -jar MaltOptimizer.jar -p 3 -m <malt_path> -c <trainingCorpus>
-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 3: FEATURE SELECTION

MaltOptimizer is going to perform the following feature selection experiments:
1. Tune the window of POSTAG n-grams over the parser state.
2. Tune the window of FORM features over the parser state.
3. Tune DEPREL and POSTAG features over the partially built dependency tree.
4. Add POSTAG and FORM features over the input string.
5. Add CPOSTAG, FEATS, and LEMMA features if available.
6. Add conjunctions of POSTAG and FORM features.
-----------------------------------------------------------------------------
1. Tuning the window of POSTAG n-grams ... 

  rm InputColumn(POSTAG,Stack[1])
  add InputColumn(POSTAG, Stack[2])
New best feature model: forwardStackPostag1.xml
Incremental LAS improvement: + 0.079% (92.37%)
  add InputColumn(POSTAG, Stack[3])
  rm InputColumn(POSTAG,Input[3])
  add InputColumn(POSTAG, Input[4])
New best feature model: forwardInputPostag1.xml
Incremental LAS improvement: + 0.109% (92.48%)
  add InputColumn(POSTAG, Input[5])

Best feature model: forwardInputPostag1.xml
-----------------------------------------------------------------------------
2. Tuning the window of FORM features ... 

  rm InputColumn(FORM,Stack[0])
  add InputColumn(FORM, Stack[1])
  rm InputColumn(FORM,Input[1])
  add InputColumn(FORM, Input[2])
  rm InputColumn(FORM, head(Stack[0]))
New best feature model: backwardHeadIterative.xml
Incremental LAS improvement: + 0.019% (92.50%)

Best feature model: backwardHeadIterative.xml
-----------------------------------------------------------------------------
3. Tuning dependency tree features ... 

  rm Merge3(InputColumn(POSTAG, Stack[0]), OutputColumn(DEPREL, ldep(Stack[0])), OutputColumn(DEPREL, rdep(Stack[0])))
  rm Merge(InputColumn(POSTAG, Stack[0]), OutputColumn(DEPREL, Stack[0]))
  rm Merge(InputColumn(POSTAG, Input[0]), OutputColumn(DEPREL, ldep(Input[0])))
  add InputColumn(POSTAG, ldep(Stack[0]))
New best feature model: replicateDeprelPostagSubtractingFeature2.xml
Incremental LAS improvement: + 0.109% (92.61%)
  add InputColumn(POSTAG, rdep(Stack[0]))

Best feature model: replicateDeprelPostagSubtractingFeature2.xml
-----------------------------------------------------------------------------
4. Adding string features ... 

  add InputColumn(POSTAG,pred(Stack[0]))
  add InputColumn(POSTAG,succ(Stack[0]))
  add InputColumn(POSTAG,pred(Input[0]))
  add InputColumn(POSTAG,succ(Input[0]))

Best feature model: replicateDeprelPostagSubtractingFeature2.xml
-----------------------------------------------------------------------------
5. Adding CPOSTAG, FEATS, and LEMMA features ... 

Adding LEMMA features ...
  add InputColumn(LEMMA, Input[0])
  add InputColumn(LEMMA, Stack[0])

Adding FEATS features ... 
  add Split(InputColumn(FEATS, Input[0]),|)
  add Split(InputColumn(FEATS, Stack[0]),|)

Best feature model: replicateDeprelPostagSubtractingFeature2.xml
-----------------------------------------------------------------------------
6. Adding conjunctions of POSTAG and FORM features... 

  add Merge(InputColumn(POSTAG, Input[0]), InputColumn(FORM, Input[0]))
  add Merge(InputColumn(POSTAG, Input[1]), InputColumn(FORM, Input[1]))
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Input[0])
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Input[1])
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[0])
  add Merge(InputColumn(POSTAG, Input[0]), InputColumn(FORM, Input[0]))
  add Merge(InputColumn(POSTAG, Input[0]), InputColumn(FORM, Input[1]))
  add Merge(InputColumn(POSTAG, Input[0]), InputColumn(FORM, Stack[0]))
  add Merge3(InputColumn(POSTAG, Input[0]), InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Input[0]))
  add Merge3(InputColumn(POSTAG, Input[0]), InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Input[1]))
  add Merge3(InputColumn(POSTAG, Input[0]), InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[0]))

Best feature model: replicateDeprelPostagSubtractingFeature2.xml
-----------------------------------------------------------------------------
MaltOptimizer has concluded feature selection and is going to tune the SVM cost parameter.

Testing: C=0.01
92.06(Best:92.61)
Testing: C=0.2
92.43(Best:92.61)
Testing: C=0.4
92.07(Best:92.61)
Testing: C=0.6
91.76(Best:92.61)
Testing: C=0.8
91.53(Best:92.61)

Best C value: 0.1
Incremental improvement over the baseline at the end of Phase 3: + 1.030% (92.61)
-----------------------------------------------------------------------------
MaltOptimizer has completed the feature model testing phase using your training set,
it saved the results for future use in phase3_logFile.txt. Updated MaltParser 
options can be found in phase3_optFile.txt.

